{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "349d4682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from langchain_openai import AzureChatOpenAI,AzureOpenAIEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain import hub\n",
    "import bs4,faiss\n",
    "from langchain_community.vectorstores import  FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers import MultiQueryRetriever, EnsembleRetriever\n",
    "from langchain.prompts import ChatPromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d1fd1551",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"LANGSMITH_TRACING\"]=\"true\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGSMITH_PROJECT\"]=\"advanced-rag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4c0a69a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_provider= get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "model= AzureChatOpenAI(\n",
    "    api_version=\"2024-12-01-preview\",\n",
    "    azure_endpoint=\"https://azopenai-langchain.openai.azure.com/\",\n",
    "    azure_ad_token_provider= token_provider,\n",
    "    model= \"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")\n",
    "\n",
    "embedding_model= AzureOpenAIEmbeddings(\n",
    "    api_version=\"2024-12-01-preview\",\n",
    "    azure_endpoint=\"https://azopenai-langchain.openai.azure.com/\",\n",
    "    azure_ad_token_provider= token_provider,\n",
    "    model= \"text-embedding-ada-002\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7cc124",
   "metadata": {},
   "source": [
    "##### Basic RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "90d9c32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='776f6746-28a4-4853-86c2-e343d4a550ba', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='The generative agent architecture. (Image source: Park et al. 2023)\\n\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:'),\n",
       " Document(id='1965a247-23d1-4086-9761-c6246e8ee19b', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='FAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.'),\n",
       " Document(id='174c3ae3-3ae9-45a1-8a91-af307f971169', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='This benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.'),\n",
       " Document(id='1dd6898f-6521-48bc-bdca-97aa4c869ab0', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.'),\n",
       " Document(id='5127b031-7356-4899-941c-1e19e086d672', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='}\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading\n",
    "loader= WebBaseLoader(web_path=\"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "                      bs_kwargs=dict(\n",
    "                            parse_only= bs4.SoupStrainer(\n",
    "                                class_ = (\"post-title\",\"post-content\",\"post-header\")\n",
    "                            )\n",
    "                      )\n",
    "                      )\n",
    "documents= loader.load()\n",
    "\n",
    "# Splitting\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size =1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Embedding\n",
    "\n",
    "index= faiss.IndexFlatL2(len(embedding_model.embed_query(\"Hello!\")))\n",
    "vector_store = FAISS(\n",
    "     embedding_function= embedding_model,\n",
    "     index= index,\n",
    "     docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "vector_store.add_documents(chunks)\n",
    "\n",
    "retriever = vector_store.as_retriever(search_type =\"mmr\",search_kwargs={'k': 5, 'fetch_k': 50})\n",
    "retriever.invoke(\"What is an AI Agent\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ab3726aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='53651887-3913-422a-8769-6032dc6f2128', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.'),\n",
       " Document(id='8fdd7576-ee19-4ad9-a481-47806f25051d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\n\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\\n\\n\\nIllustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)'),\n",
       " Document(id='1ff22925-9904-4697-b6b9-151eab7baf5f', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.'),\n",
       " Document(id='1d6ea59b-6853-4324-a7c0-993eab7cc405', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content=\"Prompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\n\\nThe generative agent architecture. (Image source: Park et al. 2023)\")]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr_retriever = vector_store.as_retriever(search_type =\"mmr\",search_kwargs={'k': 4, 'fetch_k': 50})\n",
    "mmr_retriever.invoke(\"What is Tree of Thoughts?\")\n",
    "\n",
    "similarity_retriever = vector_store.as_retriever(search_type =\"similarity\",search_kwargs={'k': 4})\n",
    "similarity_retriever.invoke(\"What is Tree of Thoughts?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "de6af60b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Tree of Thoughts is an extension of the Chain of Thought (CoT) prompting technique used in large language models (LLMs). It involves decomposing a problem into multiple reasoning steps and generating various thoughts at each step, creating a tree structure that can be explored using search methods like breadth-first or depth-first search. This approach allows for a more comprehensive exploration of reasoning possibilities, enhancing problem-solving capabilities in LLM-powered autonomous agents.'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RAG Chain\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in chunks)\n",
    "\n",
    "chain= (\n",
    "    { \"context\": mmr_retriever |format_docs , \"question\":RunnablePassthrough()  } \n",
    "    |prompt \n",
    "    | model \n",
    "    | StrOutputParser())\n",
    "\n",
    "chain.invoke(\"What is Tree of Thoughts?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2e6883",
   "metadata": {},
   "source": [
    "#### MultiQueryRetriever\n",
    "\n",
    "- This is further more improve the retriver process.\n",
    "- We have other techniques as well like BM25\n",
    "- This comes under Semantic Retrievers\n",
    "- As the name says we will generat multiple queries using a LLM in this process.\n",
    "\n",
    "#### Semantic Retrievers focus on understanding the underlying context of a query and documents in order to retrieve the relevant information from the database. Semantic Retrievers leverage word embeddings and sentence encoders to capture the semantic meaning of the text. Let’s look into few of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "93fd5cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['What methods can be used for breaking down tasks into smaller components?  ', 'How can tasks be effectively divided into manageable parts?  ', 'What strategies exist for decomposing tasks into sub-tasks?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id='53651887-3913-422a-8769-6032dc6f2128', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.'),\n",
       " Document(id='b6f891e6-2ddf-4a8c-b8fe-129e53ba46d5', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"'),\n",
       " Document(id='ef8eaf13-7b6d-4952-a306-f1a420c592e6', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#'),\n",
       " Document(id='809c177a-98f5-4812-a203-6c4da07dab7e', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='HNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.'),\n",
       " Document(id='3cc9bd1c-14be-4381-9265-b01f8012a066', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"')]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
    "logging.getLogger(\"langchain.retrievers.ensemble_retriever\").setLevel(logging.INFO)\n",
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=retriever, llm=model\n",
    ")\n",
    "retriever_from_llm.invoke(question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e55139",
   "metadata": {},
   "source": [
    "#### RAG-FUSION\n",
    "\n",
    "- This will be an extend of Multiqueryretriver, where the received multi docs from Multi Query is ranked with priority and then the final result will be used as content in LLM.\n",
    "- EnsembleRetriever internallyuses Reciprocal Rank Fusion algorithim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "825cb943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='53651887-3913-422a-8769-6032dc6f2128', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.'),\n",
       " Document(id='b6f891e6-2ddf-4a8c-b8fe-129e53ba46d5', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"'),\n",
       " Document(id='ef8eaf13-7b6d-4952-a306-f1a420c592e6', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#'),\n",
       " Document(id='200fa417-6c98-4424-b5c6-4f6732e04765', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='API-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.'),\n",
       " Document(id='1ff22925-9904-4697-b6b9-151eab7baf5f', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.'),\n",
       " Document(id='6ab30251-386b-44e6-97a3-c3f09c9f513c', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='inquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.'),\n",
       " Document(id='70e389ea-2bc2-418c-9107-338a20a3b246', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\n\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:')]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[mmr_retriever,similarity_retriever], weights=[0.5,0.5]\n",
    ")\n",
    "\n",
    "ensemble_retriever.invoke(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "da3f7058",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.load import dumps, loads\n",
    "#You are a helpful assistant that generates multiple search queries based on a single input query.\n",
    "rag_fusion_prompt = hub.pull(\"langchain-ai/rag-fusion-query-generation\")\n",
    "\n",
    "generate_queries = (\n",
    "    prompt \n",
    "    | model\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "\n",
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    fused_scores = {}\n",
    "    for docs in results:\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            fused_scores.setdefault(doc_str, 0)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "    for x in reranked_results:\n",
    "         print(x[0].page_content, x[1])\n",
    "            #print('\\n')\n",
    "    return reranked_results\n",
    "\n",
    "chain = generate_queries | retriever.map() | reciprocal_rank_fusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "942b7736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "}\n",
      "]\n",
      "Then after these clarification, the agent moved into the code writing mode with a different system message.\n",
      "System message: 0.016666666666666666\n",
      "Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\n",
      "\n",
      "\n",
      "Long-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\n",
      "\n",
      "Explicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\n",
      "Implicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Categorization of human memory.\n",
      "\n",
      "We can roughly consider the following mappings: 0.01639344262295082\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Document(id='5127b031-7356-4899-941c-1e19e086d672', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='}\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:'),\n",
       "  0.016666666666666666),\n",
       " (Document(id='856a483d-1f8b-496c-8472-b4f7e03481f2', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\n\\nCategorization of human memory.\\n\\nWe can roughly consider the following mappings:'),\n",
       "  0.01639344262295082)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"context\":\"\",\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3586ff58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"Answer the question based only on the following context.\n",
    "If you don't find the answer in the context, just say that you don't know.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "rag_fusion_chain = (\n",
    "     prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = rag_fusion_chain.invoke({\"context\":chain,\"question\":question})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fbcc3c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the approaches to Task Decomposition?'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
